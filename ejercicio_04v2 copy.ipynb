{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, data_shape, batch_size=16, validation_split=0.25, seed=2704):\n",
    "    print(f'Cargando imágenes desde: {directory}')\n",
    "    image_size = data_shape[0:2]\n",
    "\n",
    "    train_data = keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        validation_split = validation_split,\n",
    "        subset = \"training\",\n",
    "        # categorical hace que la salida sea sparce (one hot encoding)\n",
    "        label_mode='categorical',\n",
    "        seed = seed,\n",
    "        image_size = image_size,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True)\n",
    "    \n",
    "    valid_data = keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        validation_split = validation_split,\n",
    "        subset = \"validation\",\n",
    "        # categorical hace que la salida sea sparce (one hot encoding)\n",
    "        label_mode='categorical',\n",
    "        seed = seed,\n",
    "        image_size = image_size,\n",
    "        batch_size = batch_size)\n",
    "    \n",
    "    print(\"\\n¡Imágenes cargadas exitosamente!\")\n",
    "    print(f\"Número de clases encontradas: {len(train_data.class_names)}\")\n",
    "    print(f\"Nombres de las clases: {train_data.class_names}\")\n",
    "    \n",
    "    normalization_model = keras.layers.Rescaling(1./255)\n",
    "    print(\"Capa normalizadora creada con éxito\") \n",
    "\n",
    "    augmentation_model = keras.models.Sequential()\n",
    "    # aumentamos el número de imágenes haciendo imágenes en espejo vertical y horizontal\n",
    "    augmentation_model.add(keras.layers.RandomFlip(\"horizontal_and_vertical\")) \n",
    "    # aumentamos haciendo rotaciones de las imágenes.\n",
    "    # factor hace que demos un giro aleatorio desde -0.25 a 0.25 vueltas.\n",
    "    # fill_mode hace que rellenemos los pixeles vacíos con nearest (rellenamos con el pixel que tenemos mas cerca)\n",
    "    augmentation_model.add(keras.layers.RandomRotation(factor=0.25, fill_mode='nearest'))\n",
    "    augmentation_model.add(keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2, fill_mode='nearest'))\n",
    "    print(\"Capas de aumento de imágenes creadas con éxito\")\n",
    "    print(\"No olvides utilizar normalization_model y augmentation_model para mejorar tu base de datos\")\n",
    "\n",
    "    return train_data, valid_data, normalization_model, augmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(image, label, augmentation_model):\n",
    "    image = augmentation_model(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_normalization(image, label, normalization_model):\n",
    "    image = normalization_model(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(input_shape,num_classes):\n",
    "    backbone = keras.applications.resnet.ResNet50(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "    # usaremos un modelo previamente entrenado con sus parámetros aprendidos como funciones para entrenar nuestro nuevo modelo\n",
    "    for layer in backbone.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Construyamos nuestro nuevo modelo capa por capa. Comienza utilizando la capa secuencial; después \n",
    "    # agrega el backbone, una capa que aplana nuestra matriz en vectores y, por último, el \n",
    "    # encabezado de la clasificación.  No olvides ajustar el número de clases que tu modelo \n",
    "    # debe manejar  \n",
    "   \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(backbone) \n",
    "    model.add(keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc']) \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, validation_data, epochs=10):\n",
    "    model.fit(train_data, validation_data=validation_data, epochs=epochs, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando imágenes desde: archive\n",
      "Found 70549 files belonging to 15 classes.\n",
      "Using 52912 files for training.\n",
      "Found 70549 files belonging to 15 classes.\n",
      "Using 17637 files for validation.\n",
      "\n",
      "¡Imágenes cargadas exitosamente!\n",
      "Número de clases encontradas: 15\n",
      "Nombres de las clases: ['Apple', 'Banana', 'Carambola', 'Guava', 'Kiwi', 'Mango', 'Orange', 'Peach', 'Pear', 'Persimmon', 'Pitaya', 'Plum', 'Pomegranate', 'Tomatoes', 'muskmelon']\n",
      "Capa normalizadora creada con éxito\n",
      "Capas de aumento de imágenes creadas con éxito\n",
      "No olvides utilizar normalization_model y augmentation_model para mejorar tu base de datos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">495</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m131,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m495\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,721,423</span> (90.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,721,423\u001b[0m (90.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,711</span> (522.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m133,711\u001b[0m (522.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2234s\u001b[0m 671ms/step - acc: 0.2708 - loss: 2.3874 - val_acc: 0.3272 - val_loss: 2.2290\n",
      "Epoch 2/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1904s\u001b[0m 576ms/step - acc: 0.3251 - loss: 2.2431 - val_acc: 0.3353 - val_loss: 2.1914\n",
      "Epoch 3/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1925s\u001b[0m 582ms/step - acc: 0.3272 - loss: 2.2148 - val_acc: 0.3374 - val_loss: 2.1771\n",
      "Epoch 4/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2106s\u001b[0m 637ms/step - acc: 0.3301 - loss: 2.1960 - val_acc: 0.3410 - val_loss: 2.1753\n",
      "Epoch 5/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2715s\u001b[0m 821ms/step - acc: 0.3307 - loss: 2.1839 - val_acc: 0.3426 - val_loss: 2.1570\n",
      "Epoch 6/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2433s\u001b[0m 735ms/step - acc: 0.3343 - loss: 2.1704 - val_acc: 0.3435 - val_loss: 2.1366\n",
      "Epoch 7/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3334s\u001b[0m 1s/step - acc: 0.3362 - loss: 2.1623 - val_acc: 0.3550 - val_loss: 2.1526\n",
      "Epoch 8/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1901s\u001b[0m 575ms/step - acc: 0.3373 - loss: 2.1526 - val_acc: 0.3531 - val_loss: 2.1730\n",
      "Epoch 9/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1920s\u001b[0m 581ms/step - acc: 0.3387 - loss: 2.1494 - val_acc: 0.3535 - val_loss: 2.1760\n",
      "Epoch 10/10\n",
      "\u001b[1m3307/3307\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1904s\u001b[0m 576ms/step - acc: 0.3401 - loss: 2.1367 - val_acc: 0.3562 - val_loss: 2.1707\n"
     ]
    }
   ],
   "source": [
    "path = \"archive\"\n",
    "data_shape = (150, 150, 3)\n",
    "batch_size=16\n",
    "validation_split=0.25\n",
    "seed=2704\n",
    "\n",
    "# Cargamos los datos\n",
    "train_data, valid_data, normalization_model, augmentation_model = load_images(directory=path, data_shape=data_shape, batch_size=batch_size, validation_split=validation_split, seed=seed)\n",
    "num_classes = len(train_data.class_names)\n",
    "\n",
    "# Tratamiento a datos de entrenamiento\n",
    "train_data = train_data.map(lambda image, label: image_normalization(image, label, normalization_model))\n",
    "train_data = train_data.map(lambda image, label: image_augmentation(image, label, augmentation_model))\n",
    "\n",
    "# Tratamiento a datos de validación\n",
    "valid_data = valid_data.map(lambda image, label: image_normalization(image, label, normalization_model))\n",
    "\n",
    "# Creamos modelo ResNet\n",
    "model = create_resnet_model(input_shape=data_shape, num_classes=num_classes)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model = train_model(model=model, train_data=train_data, validation_data=valid_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
